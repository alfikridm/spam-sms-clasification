{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Sastrawi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0addc12ad102>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mSastrawi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStemmerFactory\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStemmerFactory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mSastrawi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStopWordRemover\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStopWordRemoverFactory\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStopWordRemoverFactory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Sastrawi'"
     ]
    }
   ],
   "source": [
    "#load datasets, library, etc\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pip\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import string\n",
    "import re\n",
    "from urlextract import URLExtract\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function collection\n",
    "def replace_multiple(mainString, toBeReplaces, newString):\n",
    "    for elem in toBeReplaces :\n",
    "        if elem in mainString :\n",
    "            mainString = mainString.replace(elem, newString)\n",
    "    return  mainString\n",
    "\n",
    "def get_emails(str):\n",
    "    regex = r'([\\w0-9._-]+@[\\w0-9._-]+\\.[\\w0-9_-]+)'\n",
    "    #(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\n",
    "    return re.findall(regex, str, re.M|re.I)\n",
    "\n",
    "def get_arr_most_occured_word_using_counter(ds):\n",
    "    list_spam_msg = ds.loc[ds['class'] == 0].clean_msg\n",
    "    string_spam_msg = list_spam_msg.str.cat(sep=', ')\n",
    "    string_spam_msg_splitted = string_spam_msg.split()\n",
    "    counter = Counter(string_spam_msg_splitted)\n",
    "    return counter\n",
    "\n",
    "def vektor(ds, feature):\n",
    "    token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "    cv = CountVectorizer(lowercase=True,ngram_range=(1,1),tokenizer=token.tokenize, vocabulary=feature)\n",
    "    # CountVectorizer(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, \n",
    "    #                 lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, \n",
    "    #                 token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), analyzer=’word’, \n",
    "    #                 max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, \n",
    "    #                 dtype=<class ‘numpy.int64’>)[source]\n",
    "    text_counts=cv.fit_transform(ds['clean_msg'])\n",
    "    return cv, text_counts\n",
    "\n",
    "# select datasets and reformat\n",
    "def select_dataset(ds):\n",
    "    if(ds == 1):\n",
    "        df = pd.read_csv('datasets\\singlish.txt', sep=\"\\t\", header=None, names = [\"class\", \"msg\"])\n",
    "        df = df[['msg', 'class']] # reorder index\n",
    "        df['class'] = df['class'].replace(['ham'], 0).replace(['spam'], 1) # represents class using number\n",
    "        df.language = 'english'\n",
    "        df.stop_words = nltk.corpus.stopwords.words(df.language)\n",
    "    elif(ds == 2):\n",
    "        df = pd.read_csv('datasets\\indo.csv')\n",
    "        df.columns = ['msg', 'class'] #rename\n",
    "        df['class'] = df['class'].replace([1,2], 1) # promo & fraud = spam (1)\n",
    "        df.language = 'indonesian'\n",
    "        df.stop_words = nltk.corpus.stopwords.words(df.language)\n",
    "    else:\n",
    "        return false\n",
    "    return df\n",
    "\n",
    "models = {\n",
    "\"knn\": KNeighborsClassifier(n_neighbors=1),\n",
    "\"naive_bayes\": GaussianNB(),\n",
    "\"decision_tree\": DecisionTreeClassifier(),\n",
    "\"random_forest\": RandomForestClassifier(n_estimators=100),\n",
    "\"multi_nb\": MultinomialNB(),\n",
    "\"bernoulli_nb\":BernoulliNB(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# preprocessing text\n",
    "extractor = URLExtract()\n",
    "lem = nltk.WordNetLemmatizer()\n",
    "porter = nltk.PorterStemmer()\n",
    "indo_factory_stemmer = StemmerFactory()\n",
    "indo_factory_stopwords = StopWordRemoverFactory()\n",
    "indo_stemmer = indo_factory_stemmer.create_stemmer()\n",
    "indo_stopwords = indo_factory_stopwords.create_stop_word_remover()\n",
    "\n",
    "def preproccessing_data(ds):\n",
    "    processed_msg = []\n",
    "    for msg in ds['msg']:\n",
    "        clean_msg = msg.lower()\n",
    "        # tokenization\n",
    "        # spelling correction\n",
    "        # masking\n",
    "        clean_msg = replace_multiple(clean_msg, get_emails(clean_msg), ' EMAILADDRESS ')\n",
    "        clean_msg = replace_multiple(clean_msg, extractor.find_urls(msg), ' WEBADDRESS ')\n",
    "        clean_msg = re.sub(r\"(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:']))\", ' WEBADDRESS ',clean_msg) # ^ not quite enough\n",
    "\n",
    "        clean_msg = re.sub(r\"(?:\\*\\d+)+#\", ' USSDCODE ',clean_msg)\n",
    "        clean_msg = re.sub(r\"\\s+(?=\\d{2}(?:\\d{2})?-\\d{1,2}-\\d{1,2})\", ' DATETIME ',clean_msg)\n",
    "        clean_msg = re.sub(r\"(?:[rp\\£\\$\\€]{1}[,\\d]+.?\\d*)\",' PRICEOFFER ',clean_msg)\n",
    "        clean_msg = re.sub(r\"(rp|USD|EUR|€|\\$)\\s?(\\d{1,3}(?:[.,]\\d{3})*(?:[.,]\\d{2}))|(\\d{1,3}(?:[.,]\\d{3})*(?:[.,]\\d{2})?)\\s?(USD|EUR|€|\\$|Rp|rb|ribu)\",' PRICEOFFER ',clean_msg)\n",
    "        clean_msg = re.sub(r\"(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?\", ' PHONENUMBER ', clean_msg)\n",
    "        clean_msg = re.sub(r\"\\d+(\\.\\d+)?\", ' SOMENUMBER ',clean_msg)\n",
    "        # striping symbols\n",
    "        clean_msg = clean_msg.translate(str.maketrans(\" \",\" \", string.punctuation))\n",
    "        # from ppt [not implemented (yet), here just in case]\n",
    "    #     clean_msg = re.sub(r\"\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b\", 'PHONENUMBER',clean_msg\n",
    "    #     clean_msg = re.sub(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", 'EMAILADDRESS',clean_msg)\n",
    "    #     clean_msg = re.sub(r'[^\\w]', ' ', clean_msg) # white space? dunno for what\n",
    "        if(ds.language == 'english'):\n",
    "            # lemmatizer\n",
    "            clean_msg = ''.join([lem.lemmatize(word) for word in clean_msg]) # not really works\n",
    "            # stemming\n",
    "            clean_msg = ''.join([porter.stem(word) for word in clean_msg])\n",
    "            # stop words\n",
    "            stop_words = set(stopwords.words(ds.language)) \n",
    "            found_stop_words = [w for w in word_tokenize(clean_msg) if w in stop_words] \n",
    "            clean_msg = replace_multiple(clean_msg, found_stop_words, '')\n",
    "        else: # indo\n",
    "            # stemming\n",
    "            clean_msg   = indo_stemmer.stem(clean_msg)\n",
    "            # stop words\n",
    "            stop_words = set(stopwords.words(ds.language))\n",
    "            clean_msg = indo_stopwords.remove(clean_msg) # sastrawi\n",
    "        processed_msg.append(clean_msg)\n",
    "    ds['clean_msg'] = processed_msg\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_most_occured_word_manual(ds):\n",
    "    msg_string =' '.join(ds.clean_msg) # all clean msg row into one string\n",
    "    msg_string_list = msg_string.lower().split() # split string into array\n",
    "    unique_words = set(msg_string_list)\n",
    "    dictwords = dict()\n",
    "    for words in unique_words:\n",
    "        dictwords[words] = [msg_string_list.count(words)]\n",
    "    real_msg = ds[ds['class'] == 0]\n",
    "    spam_msg = ds[ds['class'] == 1]\n",
    "    string_real_msg = ' '.join(real_msg.clean_msg)\n",
    "    string_spam_msg = ' '.join(spam_msg.clean_msg)\n",
    "    list_real_msg = string_real_msg.lower().split()\n",
    "    list_spam_msg = string_spam_msg.lower().split()\n",
    "    for words in unique_words :\n",
    "        count_real_msg =list_real_msg.count(words)\n",
    "        count_spam_msg =list_spam_msg.count(words)\n",
    "        dictwords[words].append(count_real_msg)\n",
    "        dictwords[words].append(count_spam_msg)\n",
    "        dictwords[words].append(count_spam_msg-count_real_msg)\n",
    "    sortfreq = sorted(dictwords.items(), key=lambda x: x[1][3], reverse=True)\n",
    "    dictfreq = sortfreq[:100]\n",
    "    return dictfreq\n",
    "# [count of word occured (all), count of word occured (real), count of word occured (spam), spam {minus} real]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# singlish datasets\n",
    "df = select_dataset(1)\n",
    "df = preproccessing_data(df)\n",
    "mo st_occured_word_counter = get_arr_most_occured_word_using_counter(df)\n",
    "most_occured_word_dictionary = get_dict_most_occured_word_manual(df)\n",
    "# df['clean_msg'].head(10)\n",
    "# export_excel = df.to_excel (r'export_dataframe.xlsx', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indo datasets\n",
    "dfX = select_dataset(2)\n",
    "dfX = preproccessing_data(dfX)\n",
    "most_occured_word_counterX = get_arr_most_occured_word_using_counter(dfX)\n",
    "most_occured_word_dictionaryX = get_dict_most_occured_word_manual(dfX)\n",
    "\n",
    "# df['clean_msg'].head(10)\n",
    "# export_excel = df.to_excel (r'export_dataframe.xlsx', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# feature = ['somenumber','phonenumber','priceoffer','free','txt','cll','webaddress','mobile','prize','clim','urgent','service','reply','text','claim','win']\n",
    "dictionary = most_occured_word_dictionary[:80]\n",
    "feature = [i[0] for i in dictionary]\n",
    "cv, text_counts = vektor(df, feature)\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_counts, df['class'], test_size=0.2, random_state=1)\n",
    "clf = MultinomialNB().fit(x_train, y_train)\n",
    "predicted = clf.predict(x_test)\n",
    "print('Multinomial NB report summary datasets singlish : ')\n",
    "print(classification_report(y_test, predicted,target_names=['not spam','spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dictionaryX = most_occured_word_dictionaryX[:30]\n",
    "featureX = [i[0] for i in dictionaryX]\n",
    "additional = ['rek','tukar','toyota','juta','raih','yaris','bpk','extra','operator','mendptkan','nama']\n",
    "featureX = featureX + additional\n",
    "cvX, text_countsX = vektor(dfX, featureX)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_countsX, dfX['class'], test_size=0.2, random_state=1)\n",
    "clfX = MultinomialNB().fit(X_train, Y_train)\n",
    "predictedX = clfX.predict(X_test)\n",
    "print('Multinomial NB report summary datasets indo : ')\n",
    "print(classification_report(Y_test, predictedX,target_names=['not spam','spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_name=\"knn\"\n",
    "model = models[model_name]\n",
    "cvX, text_countsX = vektor(dfX, featureX)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_countsX, dfX['class'], test_size=0.2, random_state=1)\n",
    "clfX = model.fit(X_train, Y_train)\n",
    "predictedX = clfX.predict(X_test)\n",
    "print(model_name, 'report summary datasets indo : ')\n",
    "print(classification_report(Y_test, predictedX,target_names=['not spam','spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_name=\"decision_tree\"\n",
    "model = models[model_name]\n",
    "cvX, text_countsX = vektor(dfX, featureX)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_countsX, dfX['class'], test_size=0.2, random_state=1)\n",
    "clfX = model.fit(X_train, Y_train)\n",
    "predictedX = clfX.predict(X_test)\n",
    "print(model_name, 'report summary datasets indo : ')\n",
    "print(classification_report(Y_test, predictedX,target_names=['not spam','spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_name=\"random_forest\"\n",
    "model = models[model_name]\n",
    "cvX, text_countsX = vektor(dfX, featureX)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_countsX, dfX['class'], test_size=0.2, random_state=1)\n",
    "clfX = model.fit(X_train, Y_train)\n",
    "predictedX = clfX.predict(X_test)\n",
    "print(model_name, 'report summary datasets indo : ')\n",
    "print(classification_report(Y_test, predictedX,target_names=['not spam','spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_name=\"bernoulli_nb\"\n",
    "model = models[model_name]\n",
    "cvX, text_countsX = vektor(dfX, featureX)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_countsX, dfX['class'], test_size=0.2, random_state=1)\n",
    "clfX = model.fit(X_train, Y_train)\n",
    "predictedX = clfX.predict(X_test)\n",
    "print(model_name, 'report summary datasets indo : ')\n",
    "print(classification_report(Y_test, predictedX,target_names=['not spam','spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
